{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install packages (other versions may work, but these are the versions I had when I\n",
        "# tested this script). If running on google colab, it may be necessary to install\n",
        "# these packages each time a new runtime is connected\n",
        "!pip install pytorch_lightning==1.8.0\n",
        "!pip install numpy==1.21.6\n",
        "!pip install pandas==1.3.5\n",
        "!pip install torch==1.12.1\n",
        "!pip install torchtext==0.13.1\n",
        "!pip install seaborn==0.11.2\n",
        "!pip install matplotlib==3.2.2\n",
        "!pip install scikit-learn==1.0.2"
      ],
      "metadata": {
        "id": "-uXLSlHjCKx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhG83KF5ubw3"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import pytorch_lightning as pl\n",
        "from collections import OrderedDict\n",
        "from torchtext import vocab # This package can give problems sometimes, it may be necessary to downgrade to a specific version\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from random import choice\n",
        "import seaborn as sns\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bqCHVy0u1mX"
      },
      "outputs": [],
      "source": [
        "# setup torchtext vocab to map AAs to indices, usage is aa2ind(list(AAsequence))\n",
        "AAs = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "WT = \"MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\"\n",
        "WT_list = list(WT)\n",
        "\n",
        "# aa2ind is an ordered dict with keys corresponding to one letter amino acid\n",
        "# codes and values corresponding to an index. Later, these indices will be\n",
        "# used for encoding amino acids into vectors\n",
        "aa2ind = vocab.vocab(OrderedDict([(a, 1) for a in AAs]))\n",
        "aa2ind.set_default_index(20) # set unknown charcterers to gap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibVAUZlDu6QB"
      },
      "outputs": [],
      "source": [
        "# Set up embedding with aaindex. aaindex is what we will use to convert\n",
        "# the indicies from aa2ind to a vector of floats that describes each amino acid\n",
        "# and will serve as inputs to the neural network.\n",
        "\n",
        "# get aa index embedding\n",
        "aaindex = np.array([[float(f) for f in l.split(',')[1:]] for l in open('pca-19_raw.csv').read().strip().split('\\n')[1:]])\n",
        "aaindex = (aaindex - aaindex.mean(0))/aaindex.std(0) # standardize\n",
        "aaindex = np.vstack([aaindex,np.zeros((1,19))]) # add final row to include gap -\n",
        "aaindex = torch.from_numpy(aaindex).float() \n",
        "ncomp = 6 # number of principal components\n",
        "aaindex = aaindex[:,:ncomp]\n",
        "\n",
        "### aaindex utilizes pca-19, which is a file with vectors representing each\n",
        "# amino acid. These vectors were determined by dimensionality reduction (PCA)\n",
        "# of the physiochemical properties of all amino acids. Emperically, we have found\n",
        "# that the first six components of this analysis capture ~99.9% of amino acid \n",
        "# properties, which is why ncomp is set to 6, but this is a hyperparameter\n",
        "# that can be tuned to modify model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRto_gCou-hz"
      },
      "outputs": [],
      "source": [
        "# The funcitons in this cell are for processing the labeled data into a format\n",
        "# that is more useful for learning. \n",
        "\n",
        "# Mutate the WT sequence and output an array of sequences with coresponding scores\n",
        "def mutate(np_mutations):\n",
        "    list_updated = []\n",
        "    count = 0\n",
        "    for i in range(len(np_mutations)):\n",
        "        try: \n",
        "            muts = np_mutations[i].split(',')\n",
        "        except:\n",
        "            muts = np_mutations[i]\n",
        "            \n",
        "        # Go through each mutation (there are one or two)\n",
        "        mut_list = list(WT)\n",
        "        for mut in muts:\n",
        "            if len(mut) == 3:\n",
        "                final_index = int(mut[1]) - 1\n",
        "                final_AA = mut[2]\n",
        "            elif len(mut) == 4:\n",
        "                final_index = int(mut[1:3]) - 1\n",
        "                final_AA = mut[3]\n",
        "\n",
        "            mut_list[final_index] = final_AA\n",
        "        # Append mutated sequence and score\n",
        "        list_updated.append(mut_list)\n",
        "        \n",
        "    return list_updated\n",
        "\n",
        "# Fix indexing in variant/mutation entries\n",
        "# this is only necessary if there are issues with 0 v 1 based indexing\n",
        "def convert_indexing(variants, offset):\n",
        "    \"\"\" convert between 0-indexed and 1-indexed \"\"\"\n",
        "    converted = [\",\".join([\"{}{}{}\".format(mut[0], int(mut[1:-1]) + offset, mut[-1])\n",
        "                           for mut in v.split(\",\")])\n",
        "                 for v in variants]\n",
        "    return converted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8OzUn_hwxVY"
      },
      "outputs": [],
      "source": [
        "# torch.set_num_threads(4) sets the number of threads in use to 4.\n",
        "# this line if very important if running on the server because pytorch is \n",
        "# greedy and will steal all the resources. It is not necessary if running on \n",
        "# colab or a personal computer, but it won't break anything if its included\n",
        "torch.set_num_threads(4) \n",
        "\n",
        "# The following loads in the experimentally collected data (thermostability of\n",
        "# gb1 mutants for this example) and calls above-defined funcitons for processing\n",
        "df = pd.read_csv(\"gb1.tsv\", sep='\\t')\n",
        "df.variant = convert_indexing(df.variant,1)    \n",
        "AA_seq_lists = mutate(list(df['variant'].copy()))\n",
        "AA_seq_lists2 = [str(\"\".join(AA_seq_lists[j])) for j in range(len(AA_seq_lists))]\n",
        "\n",
        "# Add column of full amino acid sequences. The input to the neural network will\n",
        "# be full sequences converted into numerical vectors. \n",
        "df['AA_sequences'] = AA_seq_lists2\n",
        "\n",
        "# This shuffles the dataset (pytorch should do this later but its here because\n",
        "# I'm paranoid). You can change frac to a smaller number like 0.1 to train\n",
        "# on only a portion of the dataset and speed up training\n",
        "df = df.sample(frac = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nlpq3FHvOEp"
      },
      "outputs": [],
      "source": [
        "# These three classes are the core of the neural network architecture for pytorch\n",
        "# pytorch is very modular and adaptable, so these classes can be applied to \n",
        "# other datasets with minimal modifications.\n",
        "\n",
        "# SeqFcnDataset is a helper class for loading and formating the data\n",
        "class SeqFcnDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A custom PyTorch dataset for protein sequence-function data\"\"\"\n",
        "    \n",
        "    def __init__(self, data_frame):\n",
        "        self.data_df = data_frame\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # index the data frame \n",
        "        df_row = self.data_df.iloc[idx]\n",
        "        # Note that the line below only converts the sequences to indices, the\n",
        "        # conversion using physiochemical properties (aaindex) occurs later\n",
        "        # because this script allows the conversion defenitions from index to \n",
        "        # vector to change/be learned.\n",
        "        sequence = torch.tensor(aa2ind(list(df_row.AA_sequences))) # input\n",
        "        score = torch.tensor(df_row.score) # output\n",
        "        return sequence, score.float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "\n",
        "# ProtDataModule splits the data into three different datasets:\n",
        "# training: this is the data that the model will learn from \n",
        "# validation/development: the model will not learn from this data, but you do\n",
        "# make predictions on this dataset throughout training. The performance on this\n",
        "# data can be informative as to whether the neural network is overfitting and \n",
        "# can be useful for optimizing hyperparameters\n",
        "# test: the test dataset is not used until the end. This provided us an unbiased\n",
        "# assesment of model performance.\n",
        "class ProtDataModule(pl.LightningDataModule):\n",
        "    \"\"\"A PyTorch Lightning Data Module to handle data splitting\"\"\"\n",
        "\n",
        "    def __init__(self, data_frame, batch_size):\n",
        "        super().__init__()\n",
        "        self.data_df = data_frame\n",
        "        self.batch_size = batch_size\n",
        "        train_val_test_split = [0.8, 0.1, 0.1] # What fractions of the full dataset to allocate to each subdataset\n",
        "        n_train_val_test = np.round(np.array(train_val_test_split)*len(data_frame)).astype(int)\n",
        "        if sum(n_train_val_test)<len(data_frame): n_train_val_test[0] += 1 # necesary when round is off by 1\n",
        "        if sum(n_train_val_test)>len(data_frame): n_train_val_test[0] -= 1 \n",
        "        self.train_idx, self.val_idx, self.test_idx = data_utils.random_split(range(len(data_frame)),n_train_val_test)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # prepare_data is called from a single GPU. Do not use it to assign state (self.x = y)\n",
        "        # use this method to do things that might write to disk or that need to be done only from a single process\n",
        "        # in distributed settings.\n",
        "        pass\n",
        "        \n",
        "    def setup(self, stage=None):\n",
        "              \n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == 'fit' or stage is None:\n",
        "            train_data_frame = self.data_df.iloc[list(self.train_idx)]\n",
        "            self.train_ds = SeqFcnDataset(train_data_frame)\n",
        "            \n",
        "            val_data_frame = self.data_df.iloc[list(self.val_idx)]\n",
        "            self.val_ds = SeqFcnDataset(val_data_frame)\n",
        "                    \n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == 'test' or stage is None:\n",
        "            test_data_frame = self.data_df.iloc[list(self.test_idx)]\n",
        "            self.test_ds = SeqFcnDataset(test_data_frame)\n",
        "            # print(self.test_ds)\n",
        "            \n",
        "    def train_dataloader(self):\n",
        "        return data_utils.DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data_utils.DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return data_utils.DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# PTLModule is the actual neural network. During training and validation,\n",
        "# self(sequence) will call the forward fucntion. The dimensions used within\n",
        "# the forward funciton must match those defined in __init__. The first step in\n",
        "# the forward funciton is to use aaindex to convert from an amino acid index\n",
        "# to a represenation described by physiochemical properties. \n",
        "# This class can be really confusing because many funcitons are called by pytorch\n",
        "# so while you may not see training_step get called (as an example), pytorch is\n",
        "# doing this behind the scenes.\n",
        "class PTLModule(pl.LightningModule):\n",
        "    \"\"\"PyTorch Lightning Module that defines model and training\"\"\"\n",
        "        \n",
        "    def __init__(self, slen, ks, learning_rate, epochs, ncomp):\n",
        "        super().__init__()\n",
        "      \n",
        "        # define network\n",
        "        self.embed = nn.Embedding.from_pretrained(aaindex, freeze=False)\n",
        "        # self.embed = nn.Embedding(21,ncomp) # Use only ncomp components from the embedding\n",
        "        self.slen = slen #input sequence length\n",
        "        self.ndim = self.embed.embedding_dim # dimensions of AA embedding\n",
        "        self.ks = ks # kernel size describes how many positions the neural network sees in each convolution\n",
        "        conv_out_dim = 4*self.ndim\n",
        "        self.nparam = slen*conv_out_dim # desired (flattened) output size for last convolutional layer\n",
        "\n",
        "        # Alternative way to calculate nparam: set padding=0 and have ndim change with each convolution\n",
        "        # self.nparam = (self.slen-2*(self.ks-1))*(4*self.ndim) # each convolution reduces slen by ks-1, multiply by the # output channels \n",
        "\n",
        "        self.enc_conv_1 = torch.nn.Conv1d(in_channels= self.ndim, out_channels=2*self.ndim, kernel_size=ks, padding=2) # num dim, batch size, ks\n",
        "        self.enc_conv_2 = torch.nn.Conv1d(in_channels= 2*self.ndim, out_channels=conv_out_dim, kernel_size=ks, padding=2) \n",
        "        self.linear1 = nn.Linear(self.nparam, 100)\n",
        "        self.linear2 = nn.Linear(100,1)\n",
        "\n",
        "        # loss function \n",
        "        self.loss = nn.MSELoss()\n",
        "        \n",
        "        # lr\n",
        "        self.learning_rate = learning_rate\n",
        "        self.save_hyperparameters() # log hyperparameters to file\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Convert indexed representation to aaindex physichemical representation\n",
        "        x = self.embed(x)\n",
        "\n",
        "        # Convolutional layers block\n",
        "        x = x.permute(0,2,1) # swap length and channel dims        \n",
        "        x = self.enc_conv_1(x)        \n",
        "        x = F.leaky_relu(x) # this is an activation fucniton and is the non-linear component of a neural network\n",
        "        x = self.enc_conv_2(x)\n",
        "        # -1 is like a wildcard, it is set at whatever makes the dimensions work, in this case it corresponds to the batch size\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        # Fully connected layers block\n",
        "        x = x.view(-1,self.nparam) # flatten (input for linear/FC layers must be 1D)\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x) # this is an activation fucniton and is the non-linear component of a neural network\n",
        "        x = x.view(-1,100)\n",
        "        x = self.linear2(x)\n",
        "        x = x.view(-1)\n",
        "\n",
        "        return x\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        sequence,score = batch   \n",
        "        output = self(sequence)\n",
        "        loss = self.loss(output,score)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True, on_step = False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        sequence,score = batch\n",
        "        output = self(sequence)\n",
        "        loss = self.loss(output,score)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_step = False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        sequence,score = batch\n",
        "        output = self(sequence)\n",
        "        return output\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "    \n",
        "    def predict(self, AAsequence):\n",
        "        ind = torch.tensor(aa2ind(list(AAsequence)))\n",
        "        x = ind.view(1,-1) # add batch dimension\n",
        "        pred = self(x)        \n",
        "        return float(pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLBDlpGpvDg2"
      },
      "outputs": [],
      "source": [
        "# define hyperparameters: changing these can sometimes change model \n",
        "# performance or training time\n",
        "batch_size = 64\n",
        "slen = len(WT)\n",
        "learning_rate=0.0001\n",
        "ks = 5\n",
        "epochs = 10\n",
        "\n",
        "# dm an instance of the class defined above, see notes above for its purpose\n",
        "dm = ProtDataModule(df,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwkvm5wOvJ3D"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = PTLModule(slen, ks, learning_rate, epochs, ncomp)\n",
        "\n",
        "# logger is a class instance that stores performance data to a csv after each epoch\n",
        "logger = CSVLogger('logs', name='linear')\n",
        "\n",
        "# trainer is the class PTL uses for fitting a model and saving checkpoints\n",
        "trainer = pl.Trainer(logger=logger,max_epochs=epochs)\n",
        "\n",
        "# train the model\n",
        "trainer.fit(model,dm)\n",
        "\n",
        "# Save a checkpoint of the final epoch. Note that it is also possible to save\n",
        "# checkpoints at all epochs in case the model is overfitted by the last epoch\n",
        "trainer.save_checkpoint(\"trained_gb1_conv_model.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAmIHsk2vBxB"
      },
      "outputs": [],
      "source": [
        "# plot training and validation loss to visualize the training progress.\n",
        "# we should see that both the training and validation decrease steadily over time.\n",
        "# once the validation loss stops decreasing, this is a signal that the model\n",
        "# is starting to become overfit\n",
        "\n",
        "# load metrics from logged csv file\n",
        "version = model.logger.version\n",
        "pt_metrics = pd.read_csv('./logs/linear/version_%i/metrics.csv'%version)\n",
        "train = pt_metrics[~pt_metrics.train_loss.isna()]\n",
        "val = pt_metrics[~pt_metrics.val_loss.isna()]\n",
        "\n",
        "# plot metrics to seaborn lineplot\n",
        "sns.lineplot(x=train.epoch,y=train.train_loss) #blue\n",
        "sns.lineplot(x=val.epoch,y=val.val_loss) #orange\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_-IapVVu_ju"
      },
      "outputs": [],
      "source": [
        "# Plot predicted v true scatterplot for test data\n",
        "\n",
        "# define the training and test dataframes using the dm splits\n",
        "train_data_frame = df.iloc[list(dm.train_idx)].copy()\n",
        "test_data_frame = df.iloc[list(dm.test_idx)].copy()\n",
        "\n",
        "# Set X as the true scores and Y as the predicted scores\n",
        "X = test_data_frame['score'].tolist()\n",
        "Y = []\n",
        "for i in test_data_frame['AA_sequences']:\n",
        "    Y.append(model.predict(i))\n",
        "\n",
        "# Plot and annotate the test scatterplot\n",
        "plt.scatter(X, Y, color='red', s = 0.05)\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.xlabel(\"Actual\")\n",
        "\n",
        "plt.annotate(f\"MSE = {metrics.mean_squared_error(X, Y):.3f}\", (-5.0, 0.0))\n",
        "plt.annotate(f\"R^2 = {metrics.r2_score(X, Y):.3f}\", (-5.0, -1.0))\n",
        "print('Test performance:')\n",
        "print(f\"MSE = {metrics.mean_squared_error(X, Y):.3f}\")\n",
        "print(f\"R^2 = {metrics.r2_score(X, Y):.3f}\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsQPqyIjkrz_"
      },
      "outputs": [],
      "source": [
        "# Plot predicted v true scatterplot for training data\n",
        "\n",
        "# Set X as the true scores and Y as the predicted scores\n",
        "X2 = train_data_frame['score'].tolist()\n",
        "Y2 = []\n",
        "for i in train_data_frame['AA_sequences']:\n",
        "    Y2.append(model.predict(i))\n",
        "\n",
        "# plot only 1% of the data since there's so much\n",
        "X2 = X2[:len(X2)//100]\n",
        "Y2 = Y2[:len(Y2)//100]\n",
        "\n",
        "# Plot and annotate the training scatterplot\n",
        "plt.scatter(X2, Y2, color='blue', s = 0.05)\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.xlabel(\"Actual\")\n",
        "\n",
        "plt.annotate(f\"MSE = {metrics.mean_squared_error(X2, Y2):.3f}\", (-5.0, 0.0))\n",
        "plt.annotate(f\"R^2 = {metrics.r2_score(X2, Y2):.3f}\", (-5.0, -1.))\n",
        "print('Training performance:')\n",
        "print(f\"MSE = {metrics.mean_squared_error(X2, Y2):.3f}\")\n",
        "print(f\"R^2 = {metrics.r2_score(X2, Y2):.3f}\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Note that the test statistics are most important, but if the model performs\n",
        "# much better on the training data this can be another sign of overfitting"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}